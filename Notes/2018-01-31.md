# Procedures of Kaldi Data Processing

## Kaldi data preparations

Write a python script to prepare the data that kaldi utility cannot generate for you.
In my case, I prepare the following data:
* audio data: a list of long audio data in .wav format store in the same directory
* csv data: the annotation source file where utt_id, utterance, start_time, end_time can be extracted


### Procedures

* Prepare long audio file
* Prepare python data processing script
* Prepare Kaldi directory

The directories and data preparation process follows the guideline from [Eleanor Chodroff](https://www.eleanorchodroff.com/tutorial/kaldi/kaldi-training.html) and the [Kaldi official data preparation document](http://kaldi-asr.org/doc/data_prep.html).

Before Acoustic Model Training:

In my case, I am analyzing children's audios, so I created a directory called "child".
1. Run the prepared python script [process_audio_and_text.py]()
In ~/kaldi/egs/child/ directory 
2. After the required filds are generated
We can use Kaldi's utility to generate *spk2utt* file. (This file is specified in Kaldi's official guide to not require manual creation).

```
utils/utt2spk_to_spk2utt.pl data/train/utt2spk > data/train/spk2utt
```

3. Validate the correctness of data fromat:

```
utils/validate_data_dir.sh data/train
```

fix problematic path:

```
utils/fix_data_dir.sh data/train
```

~/kaldi_build/kaldi/egs/hkust/s5/local/hkust_data_prep.sh
4. Lexicon Preparation
I am using the following script:

```
local/wsj_prepare_dict.sh
```
*Orignially from WSJ recepice*

5. Prepare language: Phone Sets, questions, L.fst compilation

```
  utils/prepare_lang.sh data/local/dict "<unk>" data/local/lang data/lang
```

6. LM training

I have tried all three scripts
```
 local/prepare_lm.sh

local/hkust_train_lms.sh

local/fisher_train_lms.sh
```

It turns out that my dataset is too small to use Kaldi LM.
Therefore, Dan suggested switch to SRILM model.
TODO: Need to look into how SRILM works.

```
local/train_lms_srilm.sh
```
There are actually two steps in language model preparation
First, split the data set into train, dev 
Second, train the data.

In 
```
kaldi/egs/iban/s5/run.sh

```
There is a step about language model training:
```
local/prepare_lm.sh
```
https://github.com/kaldi-asr/kaldi/blob/master/egs/iban/s5/local/prepare_lm.sh
In local/prepare_lm.sh:
```
#!/bin/bash
# Copyright 2015-2016  Sarah Flora Juan
# Copyright 2016  Johns Hopkins University (Author: Yenda Trmal)
# Apache 2.0

set -e -o pipefail

# To create G.fst from ARPA language model
. ./path.sh || die "path.sh expected";

local/train_lms_srilm.sh --train-text data/train/text data/ data/srilm

nl -nrz -w10  corpus/LM/iban-bp-2012.txt | utils/shuffle_list.pl > data/local/external_text
local/train_lms_srilm.sh --train-text data/local/external_text data/ data/srilm_external

# let's do ngram interpolation of the previous two LMs
# the lm.gz is always symlink to the model with the best perplexity, so we use that

mkdir -p data/srilm_interp
for w in 0.9 0.8 0.7 0.6 0.5; do
    ngram -lm data/srilm/lm.gz  -mix-lm data/srilm_external/lm.gz \
          -lambda $w -write-lm data/srilm_interp/lm.${w}.gz
    echo -n "data/srilm_interp/lm.${w}.gz "
    ngram -lm data/srilm_interp/lm.${w}.gz -ppl data/srilm/dev.txt | paste -s -
done | sort  -k15,15g  > data/srilm_interp/perplexities.txt

# for basic decoding, let's use only a trigram LM
[ -d data/lang_test/ ] && rm -rf data/lang_test
cp -R data/lang data/lang_test
lm=$(cat data/srilm/perplexities.txt | grep 3gram | head -n1 | awk '{print $1}')
local/arpa2G.sh $lm data/lang_test data/lang_test

# for decoding using bigger LM let's find which interpolated gave the most improvement
[ -d data/lang_big ] && rm -rf data/lang_big
cp -R data/lang data/lang_big
lm=$(cat data/srilm_interp/perplexities.txt | head -n1 | awk '{print $1}')
local/arpa2G.sh $lm data/lang_big data/lang_big

# for really big lm, we should only decode using small LM
# and resocre using the big lm
utils/build_const_arpa_lm.sh $lm data/lang_big data/lang_big
exit 0;
```

7. G compilation, check LG composition

~/kaldi_build/kaldi/egs/hkust/s5/local/hkust_format_data.sh
local/hkust_format_data.sh

8. Feature extraction
```
steps/make_mfcc.sh --nj 20 --cmd "$train_cmd" $dir $dir/log $dir/data
steps/compute_cmvn_stats.sh $dir $dir/log $dir/data
```

#### Special Notes
1. It is preferred to use "-" (dash) instead of underscore to name your file

2. the utt-id should contain a prefix of speaker_id
For example, *speaker_id-utt_id*

# Annotation Results

# Alignment Results

# Kaldi Running (Evluation) Results

# Sphinx Running Results

# Overall Conclusion

