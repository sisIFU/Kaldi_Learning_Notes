## Kaldi: How to split long uttrance into short one

*The exiting problem: Split the long audio file into utterances*
[Kaldi-help Google Group solution] (https://groups.google.com/forum/#!searchin/kaldi-help/split$20long$20audio$20file%7Csort:date/kaldi-help/jiff9IstTmg/vqDgzACbCwAJ)

### Proposed Solution
```
You can use the approach used in Aspire to create uniform segments and decode them.

https://github.com/kaldi-asr/kaldi/blob/master/egs/aspire/s5/local/multi_condition/prep_test_aspire.sh

local/multi_condition/create_uniform_segments.py
```


There is a solution for this.  Vimal finished it recently.

It's the script local/cleanup/segment_long_utterances.sh which works out the alignment of long transcripts to long unsegmented audio.
You then follow up by steps/cleanup/clean_and_segment_data.sh to remove any bits where the transcript is not high enough quality.

There is an example in the WSJ setup, as local/run_segmentation_long_utts.sh.

If your audio is already split into short segments and you have the text as one long piece,  then it may be best to use the 7-argument form of segment_long_utterances.sh, where you provide the text as one "text" file that has the long transcripts one per line, and an "utt2text" file that maps your shorter audio segments to the long-transcript which is belongs to.  It's OK if there is just one long-transcript.  And it's OK if it's extremely long.  This algorithm is designed to be efficient in that case.

This script has not beeen used very much so there may still be bugs.  Let us know and we'll fix them promptly.

Dan


On Wed, Jun 21, 2017 at 8:35 AM, Artur Zygad≈Ço <zygadl...@gmail.com> wrote:
Hello,

I am starting to work with Kaldi on my project which will treat on ASR.
I would like to prepare a speech corpus based on initially not aligned audio and transcriptions - something conceptually similar to what has been done for LibriSpeech.

As the initial training data, I would like to use an existing corpus.
I have some audio split into short fragments and a full text (containing all utterances from audio files combined together).
I would like to obtain ASR hypotheses and try to align them with the original transcription to get audio-text alignment.

What is the best approach to obtain the ASR hypotheses for these audio files?
Should I use the offline decoding and obtain the hypotheses from the log files?
If so, what should I put into transcriptions (text) file when preparing test data? (I do not have exact transcriptions yet, but in the beginning I don't care about the WER score, I only want the hypotheses).

My final goal is to semi-automatically prepare a new speech corpus.

Best regards,
Artur Zygadlo